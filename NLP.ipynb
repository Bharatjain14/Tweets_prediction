{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1C-F2Q7zvnYd",
        "outputId": "f9b35a67-b4b6-40db-e5e4-bf309f362dfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-0.21.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-0.21.0\n"
          ]
        }
      ],
      "source": [
        "pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kY4kELAvv00o",
        "outputId": "ca7c5c2f-277d-47f9-b1dc-5f70f8b5ea39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (4.8.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect) (0.7.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pexpect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoRYczxvwBDf",
        "outputId": "f8cd3236-058a-4f89-f8c4-50666bb030b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting expects\n",
            "  Downloading expects-0.9.0.tar.gz (27 kB)\n",
            "Building wheels for collected packages: expects\n",
            "  Building wheel for expects (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for expects: filename=expects-0.9.0-py3-none-any.whl size=18600 sha256=bb1ffd152a97b683e4acdcee82a7d1c66aa32d5a57c80159a6f1e30bef5e9e15\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/22/3f/d8c70e8db3a1c384e6d83ccb7853f8f12c75a4d20c1655a85c\n",
            "Successfully built expects\n",
            "Installing collected packages: expects\n",
            "Successfully installed expects-0.9.0\n"
          ]
        }
      ],
      "source": [
        " pip install expects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ojz4MdfwVv4"
      },
      "outputs": [],
      "source": [
        "pip install neurotic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciHDGgqBw1Av"
      },
      "outputs": [],
      "source": [
        "pip install git+https://github.com/jpgill86/neurotic.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqZPQWUGw41j"
      },
      "outputs": [],
      "source": [
        "# conda install -c conda-forge av"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "n3wtropDtx4C",
        "outputId": "4c6e37e0-d893-46c2-ac33-c2ad09f16705"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:neurotic:===========================\n",
            "INFO:neurotic:Importing neurotic 1.5.0\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-539a84aa683f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexpects\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbe_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mequal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# this project\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mneurotic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocomplete\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNGrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainTestSplit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'neurotic.nlp'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# from itertools import chain\n",
        "# import math\n",
        "# import os\n",
        "\n",
        "# # pypi\n",
        "# from dotenv import load_dotenv\n",
        "# from expects import be_true, equal, expect\n",
        "# # this project\n",
        "# from neurotic.nlp.autocomplete import NGrams, Tokenizer, TrainTestSplit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xpj6BflttykD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fhBbxYotygi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8q_3NJDtyeB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "451lkEA1tybt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcndVvIktJuV"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgq-kkyetXEd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgxr1nTdtXGu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbUL4-lFtXJe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JqcKe5QtXM9"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.data.path.append('.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "l5FvbMkVtXSD",
        "outputId": "398a9406-db23-4e1e-b208-c31a2e35bafd"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-3a8c2c9e7011>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/proxy.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data type:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of letters:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"First 300 letters of the data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/proxy.txt'"
          ]
        }
      ],
      "source": [
        "with open(\"/content/en_US.twitter.txt\", \"r\") as f:\n",
        "    data = f.read()\n",
        "print(\"Data type:\", type(data))\n",
        "print(\"Number of letters:\", len(data))\n",
        "print(\"First 300 letters of the data\")\n",
        "print(\"-------\")\n",
        "display(data[0:300])\n",
        "print(\"-------\")\n",
        "\n",
        "print(\"Last 300 letters of the data\")\n",
        "print(\"-------\")\n",
        "display(data[-300:])\n",
        "print(\"-------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O3dIuFFotXah"
      },
      "outputs": [],
      "source": [
        "\n",
        "def split_to_sentences(data):\n",
        "    \"\"\"\n",
        "    Split data by linebreak \"\\n\"\n",
        "    Args:\n",
        "    data: str\n",
        "    Returns:\n",
        "        A list of sentences\n",
        "    \"\"\"\n",
        "    sentences = data.split(\"\\n\")\n",
        "    sentences = [s.strip() for s in sentences]\n",
        "    sentences = [s for s in sentences if len(s) > 0]\n",
        "    return sentences\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1735Qw4ky8sy"
      },
      "outputs": [],
      "source": [
        "#Example\n",
        "x = \"Thunder and lightning.\\nEnter three Witches.\"\n",
        "split_to_sentences(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DEntMM--y8pT"
      },
      "outputs": [],
      "source": [
        "def tokenize_sentences(sentences):\n",
        "    tokenized_sentences = []\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.lower()\n",
        "        tokenized = nltk.word_tokenize(sentence)\n",
        "        # append the list of words to the list\n",
        "        tokenized_sentences.append(tokenized)\n",
        "\n",
        "    return tokenized_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Yfa2tXmR2PMe"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "07gLo3aEy8m8"
      },
      "outputs": [],
      "source": [
        "#Example\n",
        "sentences = [\"Sky is blue.\", \"Leaves are green.\", \"Roses are red.\"]\n",
        "tokenize_sentences(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iEtS140jy8kM"
      },
      "outputs": [],
      "source": [
        "def get_tokenized_data(data):\n",
        "    sentences = split_to_sentences(data)\n",
        "    tokenized_sentences = tokenize_sentences(sentences)\n",
        "    return tokenized_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_c_13YlVy8h4"
      },
      "outputs": [],
      "source": [
        "tokenized_data = get_tokenized_data(data)\n",
        "random.seed(87)\n",
        "random.shuffle(tokenized_data)# changes the original list/tuple/string\n",
        "train_size = int(len(tokenized_data) * 0.8)\n",
        "train_data = tokenized_data[0:train_size]\n",
        "test_data = tokenized_data[train_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9Wo9fdBHy8eX"
      },
      "outputs": [],
      "source": [
        "print(\"{} data are split into {} train and {} test set\".format(\n",
        "    len(tokenized_data), len(train_data), len(test_data)))\n",
        "\n",
        "print(\"First training sample:\")\n",
        "print(train_data[0])\n",
        "\n",
        "print(\"First test sample\")\n",
        "print(test_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-ZMsvo19y8bI"
      },
      "outputs": [],
      "source": [
        "\n",
        "def count_words(tokenized_sentences):\n",
        "    word_counts = {}\n",
        "    for sentence in range(len(tokenized_sentences)):\n",
        "        # go through tokens in the sentence\n",
        "        for token in (tokenized_sentences[sentence]):\n",
        "            if token not in word_counts.keys():\n",
        "                word_counts[token] = 1\n",
        "            else:\n",
        "                word_counts[token] += 1\n",
        "    return word_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Eh82RLBGy8Yw"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n",
        "    # appear at least 'minimum_freq' times.\n",
        "    closed_vocab = []\n",
        "    word_counts = count_words(tokenized_sentences)\n",
        "    for word, cnt in word_counts.items():\n",
        "\n",
        "        if cnt>=count_threshold:\n",
        "            closed_vocab.append(word)\n",
        "    return closed_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7HqV8hwOy8WQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
        "    vocabulary = set(vocabulary)\n",
        "\n",
        "    replaced_tokenized_sentences = []\n",
        "\n",
        "    # go through sentences\n",
        "    for sentence in tokenized_sentences:\n",
        "        replaced_sentence = []\n",
        "        # go through tokens\n",
        "        for token in sentence:\n",
        "            # Is the token in the closed vocabulary?\n",
        "            if token in vocabulary:\n",
        "                # If so, append the word to the replaced_sentence\n",
        "                replaced_sentence.append(token)\n",
        "            else:\n",
        "                # otherwise, append the unknown token\n",
        "                replaced_sentence.append(unknown_token)\n",
        "        replaced_tokenized_sentences.append(replaced_sentence)\n",
        "    return replaced_tokenized_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pEUjLeQVzoHK"
      },
      "outputs": [],
      "source": [
        "\n",
        "def preprocess_data(train_data, test_data, count_threshold):\n",
        "    vocabulary = get_words_with_nplus_frequency(train_data,count_threshold)\n",
        "    train_data_replaced=[]\n",
        "    test_data_replaced=[]\n",
        "    # replace less common words with \"<unk>\" training set\n",
        "    for sentence in range(len(train_data)):\n",
        "        parole=[]\n",
        "        for word in range(len(train_data[sentence])):\n",
        "            if train_data[sentence][word] in vocabulary:\n",
        "                parole.append(train_data[sentence][word])\n",
        "            else:\n",
        "                parole.append(\"<unk>\")\n",
        "\n",
        "        train_data_replaced.append(parole)\n",
        "    # replace less common words with \"<unk>\" test set\n",
        "    for sentence in range(len(test_data)):\n",
        "        parole_test=[]\n",
        "        for word in range(len(test_data[sentence])):\n",
        "            if test_data[sentence][word] in vocabulary:\n",
        "                parole_test.append(test_data[sentence][word])\n",
        "            else:\n",
        "                parole_test.append(\"<unk>\")\n",
        "        test_data_replaced.append(parole_test)\n",
        "    return train_data_replaced, test_data_replaced, vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJ_PaXTY28Bi"
      },
      "outputs": [],
      "source": [
        "minimum_freq = 2\n",
        "train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, test_data,minimum_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqBiC4YizqB-",
        "outputId": "d82ffddc-2295-443e-a823-8886eff47f62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First preprocessed training sample:\n",
            "['i', 'think', 'i', 'agree', 'with', 'the', 'whole', 'instagram', 'thing', 'even', 'though', 'i', 'have', 'an', 'andriod', 'cause', 'u', ',', 'cant', 'bbm', 'on', 'a', 'iphone', 'cause', 'tht', 'would', 'b', 'wack', 'as', 'hell']\n",
            "\n",
            "First preprocessed test sample:\n",
            "['thank', 'god', '#', 'friday', ':', ')']\n",
            "\n",
            "First 10 vocabulary:\n",
            "['i', 'think', 'agree', 'with', 'the', 'whole', 'instagram', 'thing', 'even', 'though']\n",
            "\n",
            "Size of vocabulary: 37595\n"
          ]
        }
      ],
      "source": [
        "print(\"First preprocessed training sample:\")\n",
        "print(train_data_processed[0])\n",
        "print()\n",
        "print(\"First preprocessed test sample:\")\n",
        "print(test_data_processed[0])\n",
        "print()\n",
        "print(\"First 10 vocabulary:\")\n",
        "print(vocabulary[0:10])\n",
        "print()\n",
        "print(\"Size of vocabulary:\", len(vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5p1L7_Izr41"
      },
      "outputs": [],
      "source": [
        "def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):\n",
        "   n_grams = {}\n",
        "   for sentence in range(len(data)):\n",
        "       sentences = [start_token]*n+list(data[sentence])+[end_token]\n",
        "       sentences = tuple(sentences)\n",
        "       for i in range(0,len(sentences)-n+1):\n",
        "           # Get the n-gram from i to i+n\n",
        "           n_gram = sentences[i:i+n]\n",
        "           # Is the n-gram in the dictionary?\n",
        "           if n_gram in n_grams.keys():\n",
        "               # Increment the count\n",
        "               n_grams[n_gram] += 1\n",
        "           else:\n",
        "               # otherwise is 1\n",
        "               n_grams[n_gram] = 1\n",
        "   return n_grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCJx607UztZS",
        "outputId": "82e25378-c5bb-4dc2-c9f0-ebc917158a6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uni-gram:\n",
            "{('<s>',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('<e>',): 2, ('this',): 1, ('dog',): 1, ('is',): 1}\n",
            "Bi-gram:\n",
            "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '<e>'): 2, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1}\n"
          ]
        }
      ],
      "source": [
        "#Example\n",
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "print(\"Uni-gram:\")\n",
        "print(count_n_grams(sentences, 1))\n",
        "print(\"Bi-gram:\")\n",
        "print(count_n_grams(sentences, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NR977MWczwAr"
      },
      "outputs": [],
      "source": [
        "\n",
        "def estimate_probability(word, previous_n_gram,\n",
        "                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
        "    if type(previous_n_gram)==list:\n",
        "        previous_n_grams = tuple(previous_n_gram)\n",
        "    else:\n",
        "        previous_n_grams = tuple([previous_n_gram])\n",
        "    previous_n_gram_count = n_gram_counts.get(previous_n_grams,0)\n",
        "    # and apply k-smoothing\n",
        "    denominator = previous_n_gram_count+vocabulary_size*1\n",
        "    n_plus1_gram =previous_n_grams+tuple([word])\n",
        "    n_plus1_gram_count = n_plus1_gram_counts.get(n_plus1_gram,0)\n",
        "\n",
        "    # and apply smoothing\n",
        "    numerator = n_plus1_gram_count+k\n",
        "    probability = numerator/denominator\n",
        "    return probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5enbQ8EzyTe"
      },
      "outputs": [],
      "source": [
        "\n",
        "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
        "\n",
        "    previous_n_gram = previous_n_gram\n",
        "    # add <e> <unk> to the vocabulary <s> is not needed since it should not appear as the next word\n",
        "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
        "    vocabulary_size = len(vocabulary)\n",
        "    probabilities = {}\n",
        "    for word in vocabulary:\n",
        "        probability = estimate_probability(word, previous_n_gram,\n",
        "                                           n_gram_counts, n_plus1_gram_counts,\n",
        "                                           vocabulary_size, k=k)\n",
        "        probabilities[word] = probability\n",
        "    return probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0TPXVhYzyQF"
      },
      "outputs": [],
      "source": [
        "\n",
        "def make_count_matrix(n_plus1_gram_counts, vocabulary):\n",
        "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
        "    n_grams = []\n",
        "    for n_plus1_gram in n_plus1_gram_counts.keys():\n",
        "        n_gram = n_plus1_gram[0:-1]\n",
        "        n_grams.append(n_gram)\n",
        "    n_grams = list(set(n_grams))\n",
        "    row_index = {n_gram:i for i, n_gram in enumerate(n_grams)}\n",
        "    col_index = {word:j for j, word in enumerate(vocabulary)}\n",
        "    nrow = len(n_grams)\n",
        "    ncol = len(vocabulary)\n",
        "    count_matrix = np.zeros((nrow, ncol))\n",
        "    for n_plus1_gram, count in n_plus1_gram_counts.items():\n",
        "        n_gram = n_plus1_gram[0:-1]\n",
        "        word = n_plus1_gram[-1]\n",
        "        if word not in vocabulary:\n",
        "            continue\n",
        "        i = row_index[n_gram]\n",
        "        j = col_index[word]\n",
        "        count_matrix[i, j] = count\n",
        "\n",
        "    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)\n",
        "    return count_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rp7WOeHAzyNu"
      },
      "outputs": [],
      "source": [
        "\n",
        "def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n",
        "   count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)\n",
        "   count_matrix += k\n",
        "   prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
        "   return prob_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYvxvWN5zyKU"
      },
      "outputs": [],
      "source": [
        "\n",
        "def calculate_perplexity(sentences, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
        "\n",
        "   n = len(list(n_gram_counts.keys())[0])\n",
        "   sentence = [\"<s>\"] * n + sentences + [\"<e>\"]\n",
        "\n",
        "   sentence = tuple(sentence)\n",
        "   N = len(sentence)\n",
        "   product_pi = 1.0\n",
        "   for t in range(n, N): # complete this line\n",
        "\n",
        "       n_gram = sentence[t-n]\n",
        "       word = sentence[t]\n",
        "       probability = estimate_probability(word, n_gram,\n",
        "                        n_gram_counts, n_plus1_gram_counts, vocabulary_size, k)\n",
        "\n",
        "\n",
        "       product_pi *= 1/probability\n",
        "\n",
        "   # Nth root of the product\n",
        "   perplexity = product_pi**(1/float(N))\n",
        "   perplexity = float(perplexity)\n",
        "\n",
        "   return perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxU3GLZyzyH8"
      },
      "outputs": [],
      "source": [
        "\n",
        "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
        "\n",
        "   n = len(list(n_gram_counts.keys())[0])\n",
        "   probabilities = estimate_probabilities(previous_n_gram,\n",
        "                                          n_gram_counts, n_plus1_gram_counts,\n",
        "                                          vocabulary, k=k)\n",
        "   suggestion = None\n",
        "   max_prob = 0\n",
        "   for word, prob in probabilities.items():\n",
        "       if start_with != None : # complete this line\n",
        "           if word.startswith(start_with)==False:\n",
        "               #If so, move onto the next word\n",
        "               continue\n",
        "       if prob>max_prob:\n",
        "           suggestion = word\n",
        "           max_prob = prob\n",
        "\n",
        "   return suggestion, max_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i31MB1X65WrX"
      },
      "outputs": [],
      "source": [
        "\n",
        "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
        "\n",
        "   n = len(list(n_gram_counts.keys())[0])\n",
        "   probabilities = estimate_probabilities(previous_n_gram,\n",
        "                                          n_gram_counts, n_plus1_gram_counts,\n",
        "                                          vocabulary, k=k)\n",
        "   suggestion = None\n",
        "   max_prob = 0\n",
        "   for word, prob in probabilities.items():\n",
        "       if start_with != None : # complete this line\n",
        "           if word.startswith(start_with)==False:\n",
        "               #If so, move onto the next word\n",
        "               continue\n",
        "       if prob>max_prob:\n",
        "           suggestion = word\n",
        "           max_prob = prob\n",
        "\n",
        "   return suggestion, max_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "n6XBTsUbzyGK",
        "outputId": "98e5925e-ed30-453d-d668-4c650a4d1ba8"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-bb9a10b0a238>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprevious_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"like\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtmp_suggest1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuggest_a_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munigram_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigram_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The previous words are 'i like',\\n\\tand the suggested word is `{tmp_suggest1[0]}` with a probability of {tmp_suggest1[1]:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-3fa9376af945>\u001b[0m in \u001b[0;36msuggest_a_word\u001b[0;34m(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k, start_with)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m    \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_gram_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m    probabilities = estimate_probabilities(previous_n_gram,\n\u001b[0m\u001b[1;32m      5\u001b[0m                                           \u001b[0mn_gram_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_plus1_gram_counts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                           vocabulary, k=k)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'previous_n_gram' is not defined"
          ]
        }
      ],
      "source": [
        "#Example\n",
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "\n",
        "previous_tokens = [\"i\", \"like\"]\n",
        "tmp_suggest1 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\n",
        "print(f\"The previous words are 'i like',\\n\\tand the suggested word is `{tmp_suggest1[0]}` with a probability of {tmp_suggest1[1]:.4f}\")\n",
        "\n",
        "print()\n",
        "#Example starts_with\n",
        "tmp_starts_with = 'c'\n",
        "tmp_suggest2 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0, start_with=tmp_starts_with)\n",
        "print(f\"The previous words are 'i like', the suggestion must start with `{tmp_starts_with}`\\n\\tand the suggested word is `{tmp_suggest2[0]}` with a probability of {tmp_suggest2[1]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlCvVqIZ0FUX"
      },
      "outputs": [],
      "source": [
        "\n",
        "  def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
        "    model_counts = len(n_gram_counts_list)\n",
        "    suggestions = []\n",
        "    for i in range(model_counts-1):\n",
        "        n_gram_counts = n_gram_counts_list[i]\n",
        "        n_plus1_gram_counts = n_gram_counts_list[i+1]\n",
        "\n",
        "        suggestion = suggest_a_word(previous_tokens, n_gram_counts,\n",
        "                                    n_plus1_gram_counts, vocabulary,\n",
        "                                    k=k, start_with=start_with)\n",
        "        suggestions.append(suggestion)\n",
        "    return suggestions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNG2CGHu0FQ-",
        "outputId": "47da0dd3-86c4-4929-882b-5e350c2c007f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing n-gram counts with n = 1 ...\n",
            "Computing n-gram counts with n = 2 ...\n",
            "Computing n-gram counts with n = 3 ...\n",
            "Computing n-gram counts with n = 4 ...\n",
            "Computing n-gram counts with n = 5 ...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "n_gram_counts_list = []\n",
        "for n in range(1, 6):\n",
        "    print(\"Computing n-gram counts with n =\", n, \"...\")\n",
        "    n_model_counts = count_n_grams(train_data_processed, n)\n",
        "    n_gram_counts_list.append(n_model_counts)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "Z8TwF19g0FPE",
        "outputId": "3c833bbf-54c8-4543-b3d7-dac70f0178a9"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-12f04180f983>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprevious_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"am\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtmp_suggest1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_suggestions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_gram_counts_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The previous words are {previous_tokens}, the suggestions are:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_suggest1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-28403cd6b283>\u001b[0m in \u001b[0;36mget_suggestions\u001b[0;34m(previous_tokens, n_gram_counts_list, vocabulary, k, start_with)\u001b[0m\n\u001b[1;32m      8\u001b[0m       suggestion = suggest_a_word(previous_tokens, n_gram_counts,\n\u001b[1;32m      9\u001b[0m                                   \u001b[0mn_plus1_gram_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                                   k=k, start_with=start_with)\n\u001b[0m\u001b[1;32m     11\u001b[0m       \u001b[0msuggestions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuggestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msuggestions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-3fa9376af945>\u001b[0m in \u001b[0;36msuggest_a_word\u001b[0;34m(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k, start_with)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m    \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_gram_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m    probabilities = estimate_probabilities(previous_n_gram,\n\u001b[0m\u001b[1;32m      5\u001b[0m                                           \u001b[0mn_gram_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_plus1_gram_counts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                           vocabulary, k=k)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'previous_n_gram' is not defined"
          ]
        }
      ],
      "source": [
        "previous_tokens = [\"i\", \"am\", \"to\"]\n",
        "tmp_suggest1 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
        "\n",
        "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
        "display(tmp_suggest1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPCiMhLJ0FL3"
      },
      "outputs": [],
      "source": [
        "previous_tokens = [\"hey\", \"how\", \"are\", \"you\"]\n",
        "tmp_suggest2 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
        "\n",
        "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
        "display(tmp_suggest2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FBkVjBt0FJU"
      },
      "outputs": [],
      "source": [
        "previous_tokens = [\"hey\", \"how\", \"are\", \"you\"]\n",
        "tmp_suggest3 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=\"d\")\n",
        "\n",
        "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
        "display(tmp_suggest3)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}